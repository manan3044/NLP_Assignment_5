{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9636743,"sourceType":"datasetVersion","datasetId":5883900}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu\nimport nltk\n\nnltk.download('punkt')\n\nimport logging\nlogging.disable(logging.WARNING)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-16T04:39:39.292229Z","iopub.execute_input":"2024-10-16T04:39:39.293035Z","iopub.status.idle":"2024-10-16T04:39:45.686100Z","shell.execute_reply.started":"2024-10-16T04:39:39.292992Z","shell.execute_reply":"2024-10-16T04:39:45.685220Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_coqa_data(file_path):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data['data']\n\n# Custom dataset class\nclass CoQADataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        context = item['story']\n        question = item['questions'][0]['input_text']\n        answer = item['answers'][0]['input_text']\n\n        # T5 format: \"question: <question> context: <context>\"\n        input_text = f\"question: {question} context: {context}\"\n        target_text = answer\n\n        # Tokenize the input and target texts\n        inputs = self.tokenizer(\n            input_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        labels = self.tokenizer(\n            target_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'labels': labels['input_ids'].flatten()  # Use labels for T5\n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:45.688037Z","iopub.execute_input":"2024-10-16T04:39:45.688756Z","iopub.status.idle":"2024-10-16T04:39:45.698341Z","shell.execute_reply.started":"2024-10-16T04:39:45.688688Z","shell.execute_reply":"2024-10-16T04:39:45.697568Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = load_coqa_data('/kaggle/input/qna-dataset/coqa-train-v1.0.json')\ntrain_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\nval_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:45.699415Z","iopub.execute_input":"2024-10-16T04:39:45.699643Z","iopub.status.idle":"2024-10-16T04:39:46.488297Z","shell.execute_reply.started":"2024-10-16T04:39:45.699612Z","shell.execute_reply":"2024-10-16T04:39:46.487587Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Initialize tokenizer and model\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\ntrain_dataset = CoQADataset(train_data, tokenizer)\nval_dataset = CoQADataset(val_data, tokenizer)\ntest_dataset = CoQADataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\ntest_loader = DataLoader(test_dataset, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:46.490172Z","iopub.execute_input":"2024-10-16T04:39:46.490399Z","iopub.status.idle":"2024-10-16T04:39:47.679613Z","shell.execute_reply.started":"2024-10-16T04:39:46.490368Z","shell.execute_reply":"2024-10-16T04:39:47.678753Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea37a5997d584e5e9431ad8a09aacbc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b0052f62a94f4196f1e785052898c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c0043a0a09e42f4b6a27241c2415536"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def train(model, train_loader, optimizer, device):\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(train_loader, desc=\"Training\")\n\n    for batch in progress_bar:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass with T5\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n        # Get the loss from the output\n        loss = outputs.loss\n\n        # Average loss across GPUs (if using multiple)\n        loss = loss.mean()\n\n        total_loss += loss.item()\n\n        # Backward pass and optimization step\n        loss.backward()\n        optimizer.step()\n\n        # Update progress bar with current loss\n        progress_bar.set_postfix({'loss': loss.item()})\n\n    # Return the average loss over all batches\n    return total_loss / len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:47.682795Z","iopub.execute_input":"2024-10-16T04:39:47.683021Z","iopub.status.idle":"2024-10-16T04:39:47.690614Z","shell.execute_reply.started":"2024-10-16T04:39:47.682987Z","shell.execute_reply":"2024-10-16T04:39:47.689668Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\nmodel = T5ForConditionalGeneration.from_pretrained('t5-base')\n\n\n# Set device and move model to device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Set optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:47.691866Z","iopub.execute_input":"2024-10-16T04:39:47.692110Z","iopub.status.idle":"2024-10-16T04:39:53.202203Z","shell.execute_reply.started":"2024-10-16T04:39:47.692074Z","shell.execute_reply":"2024-10-16T04:39:53.201264Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3abad500f4064f3cb08d07737504ab86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae56e3cff634ab581d4511e06617fca"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use DataParallel to parallelize across GPUs\nimport torch.nn as nn\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:53.203389Z","iopub.execute_input":"2024-10-16T04:39:53.203659Z","iopub.status.idle":"2024-10-16T04:39:53.217869Z","shell.execute_reply.started":"2024-10-16T04:39:53.203613Z","shell.execute_reply":"2024-10-16T04:39:53.216947Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Using 2 GPUs!\n","output_type":"stream"}]},{"cell_type":"code","source":"def validate(model, val_loader, device):\n    model.eval()\n    total_loss = 0\n    progress_bar = tqdm(val_loader, desc=\"Validating\")\n    with torch.no_grad():\n        for batch in progress_bar:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            # Forward pass with T5\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            progress_bar.set_postfix({'loss': loss.item()})\n\n    return total_loss / len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:53.219377Z","iopub.execute_input":"2024-10-16T04:39:53.219643Z","iopub.status.idle":"2024-10-16T04:39:53.227745Z","shell.execute_reply.started":"2024-10-16T04:39:53.219602Z","shell.execute_reply":"2024-10-16T04:39:53.226691Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Test function\ndef test(model, test_loader, tokenizer, device):\n    model.eval()\n    all_predictions = []\n    all_answers = []\n    progress_bar = tqdm(test_loader, desc=\"Testing\")\n    with torch.no_grad():\n        for batch in progress_bar:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            answers = batch['answer']\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            start_scores = outputs.start_logits\n            end_scores = outputs.end_logits\n\n            for i in range(input_ids.shape[0]):\n                start_index = torch.argmax(start_scores[i])\n                end_index = torch.argmax(end_scores[i])\n                prediction = tokenizer.decode(input_ids[i][start_index:end_index+1])\n                all_predictions.append(prediction)\n                all_answers.append(answers[i])\n\n    bleu_score = calculate_bleu(all_predictions, all_answers)\n    return bleu_score","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:53.229305Z","iopub.execute_input":"2024-10-16T04:39:53.229563Z","iopub.status.idle":"2024-10-16T04:39:53.240332Z","shell.execute_reply.started":"2024-10-16T04:39:53.229523Z","shell.execute_reply":"2024-10-16T04:39:53.239397Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 3\nbest_loss = float('inf')\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    train_loss = train(model, train_loader, optimizer, device)\n    val_loss = validate(model, val_loader, device)\n    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n    if val_loss < best_loss:\n        best_loss = val_loss\n        torch.save(model.state_dict(), 't5_qa_model.pth')\n        print(\"Model saved!\")\n    else:\n        print(\"Validation Loss Increased. Model Not Saved.\")\n    print(\"*\" * 50)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T04:39:53.244739Z","iopub.execute_input":"2024-10-16T04:39:53.245432Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/630 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining:   0%|          | 1/630 [00:15<2:41:41, 15.42s/it, loss=24.9]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nTraining:   1%|          | 6/630 [00:21<21:28,  2.06s/it, loss=15.3]  ","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Set the environment variable to help avoid fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate BLEU score\ndef calculate_bleu(predictions, references):\n    bleu_scores = []\n    for pred, ref in zip(predictions, references):\n        bleu_scores.append(sentence_bleu([ref.split()], pred.split()))\n    return sum(bleu_scores) / len(bleu_scores)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model\nbleu_score = test(model, test_loader, tokenizer, device)\nprint(f\"BLEU Score: {bleu_score:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a simple QA bot\ndef qa_bot(context, question):\n    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        start_scores = outputs.start_logits\n        end_scores = outputs.end_logits\n\n    start_index = torch.argmax(start_scores)\n    end_index = torch.argmax(end_scores)\n    answer = tokenizer.decode(input_ids[0][start_index:end_index+1])\n    return answer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data[0]['story']","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}