{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-22T13:29:29.776059Z","iopub.status.busy":"2024-10-22T13:29:29.775606Z","iopub.status.idle":"2024-10-22T13:29:36.117274Z","shell.execute_reply":"2024-10-22T13:29:36.116433Z","shell.execute_reply.started":"2024-10-22T13:29:29.776015Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from nltk.translate.bleu_score import sentence_bleu\n","import nltk\n","\n","nltk.download('punkt')\n","\n","import logging\n","logging.disable(logging.WARNING)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:36.119788Z","iopub.status.busy":"2024-10-22T13:29:36.119232Z","iopub.status.idle":"2024-10-22T13:29:36.129355Z","shell.execute_reply":"2024-10-22T13:29:36.128359Z","shell.execute_reply.started":"2024-10-22T13:29:36.119739Z"},"trusted":true},"outputs":[],"source":["def load_coqa_data(file_path):\n","    with open(file_path, 'r') as f:\n","        data = json.load(f)\n","    return data['data']\n","\n","# Custom dataset class\n","class CoQADataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        context = item['story']\n","        question = item['questions'][0]['input_text']\n","        answer = item['answers'][0]['input_text']\n","\n","        # T5 format: \"question: <question> context: <context>\"\n","        input_text = f\"question: {question} context: {context}\"\n","        target_text = answer\n","\n","        # Tokenize the input and target texts\n","        inputs = self.tokenizer(\n","            input_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        labels = self.tokenizer(\n","            target_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'labels': labels['input_ids'].flatten()  # Use labels for T5\n","        }"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:36.131280Z","iopub.status.busy":"2024-10-22T13:29:36.130730Z","iopub.status.idle":"2024-10-22T13:29:36.969559Z","shell.execute_reply":"2024-10-22T13:29:36.968791Z","shell.execute_reply.started":"2024-10-22T13:29:36.131237Z"},"trusted":true},"outputs":[],"source":["data = load_coqa_data('/kaggle/input/qna-dataset/coqa-train-v1.0.json')\n","train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n","val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:36.972740Z","iopub.status.busy":"2024-10-22T13:29:36.972181Z","iopub.status.idle":"2024-10-22T13:29:38.235189Z","shell.execute_reply":"2024-10-22T13:29:38.234218Z","shell.execute_reply.started":"2024-10-22T13:29:36.972692Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"262071c8ce7f45bf9ef566b5abca6f4c","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0525571811b6411c9f384183267c8f1f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a028b17e903f4eb9ba8b5f300f9791cd","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# Initialize tokenizer and model\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","\n","train_dataset = CoQADataset(train_data, tokenizer)\n","val_dataset = CoQADataset(val_data, tokenizer)\n","test_dataset = CoQADataset(test_data, tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8)\n","test_loader = DataLoader(test_dataset, batch_size=8)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:38.236886Z","iopub.status.busy":"2024-10-22T13:29:38.236575Z","iopub.status.idle":"2024-10-22T13:29:38.245335Z","shell.execute_reply":"2024-10-22T13:29:38.244231Z","shell.execute_reply.started":"2024-10-22T13:29:38.236823Z"},"trusted":true},"outputs":[],"source":["def train(model, train_loader, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","    progress_bar = tqdm(train_loader, desc=\"Training\")\n","\n","    for batch in progress_bar:\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass with T5\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","\n","        # Get the loss from the output\n","        loss = outputs.loss\n","\n","        # Average loss across GPUs (if using multiple)\n","        loss = loss.mean()\n","\n","        total_loss += loss.item()\n","\n","        # Backward pass and optimization step\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update progress bar with current loss\n","        progress_bar.set_postfix({'loss': loss.item()})\n","\n","    # Return the average loss over all batches\n","    return total_loss / len(train_loader)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:38.246600Z","iopub.status.busy":"2024-10-22T13:29:38.246398Z","iopub.status.idle":"2024-10-22T13:29:44.133382Z","shell.execute_reply":"2024-10-22T13:29:44.132635Z","shell.execute_reply.started":"2024-10-22T13:29:38.246572Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6001958249254f5ba26e02e9776700aa","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7de0b18147db4e7691d096614dfae2b0","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n","model = T5ForConditionalGeneration.from_pretrained('t5-base')\n","\n","\n","# Set device and move model to device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Set optimizer\n","optimizer = AdamW(model.parameters(), lr=5e-5)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:44.134636Z","iopub.status.busy":"2024-10-22T13:29:44.134432Z","iopub.status.idle":"2024-10-22T13:29:44.147393Z","shell.execute_reply":"2024-10-22T13:29:44.146436Z","shell.execute_reply.started":"2024-10-22T13:29:44.134606Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using 2 GPUs!\n"]}],"source":["# Use DataParallel to parallelize across GPUs\n","import torch.nn as nn\n","\n","if torch.cuda.device_count() > 1:\n","    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n","    model = nn.DataParallel(model)\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:44.148811Z","iopub.status.busy":"2024-10-22T13:29:44.148570Z","iopub.status.idle":"2024-10-22T13:29:44.157261Z","shell.execute_reply":"2024-10-22T13:29:44.156409Z","shell.execute_reply.started":"2024-10-22T13:29:44.148777Z"},"trusted":true},"outputs":[],"source":["def validate(model, val_loader, device):\n","    model.eval()\n","    total_loss = 0\n","    progress_bar = tqdm(val_loader, desc=\"Validating\")\n","    with torch.no_grad():\n","        for batch in progress_bar:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            # Forward pass with T5\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","\n","        # Average loss across GPUs (if using multiple)\n","            loss = loss.mean()\n","\n","            total_loss += loss.item()\n","\n","            progress_bar.set_postfix({'loss': loss.item()})\n","\n","    return total_loss / len(val_loader)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:44.159362Z","iopub.status.busy":"2024-10-22T13:29:44.158534Z","iopub.status.idle":"2024-10-22T13:29:44.168471Z","shell.execute_reply":"2024-10-22T13:29:44.167664Z","shell.execute_reply.started":"2024-10-22T13:29:44.159326Z"},"trusted":true},"outputs":[],"source":["def test2(model, test_loader, tokenizer, device):\n","    model.eval()\n","    all_predictions = []\n","    all_answers = []\n","    progress_bar = tqdm(test_loader, desc=\"Testing\")\n","    \n","    with torch.no_grad():\n","        for batch in progress_bar:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            answers = batch['labels'].to(device)  # Ensure answers are on the correct device\n","            \n","            # Generate outputs from the model\n","            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            # Decode predictions and append to the list\n","            for i in range(outputs.shape[0]):\n","                prediction = tokenizer.decode(outputs[i], skip_special_tokens=True)\n","                all_predictions.append(prediction)\n","                # Decoding the ground truth labels (answers) as well\n","                ground_truth = tokenizer.decode(answers[i], skip_special_tokens=True)\n","                all_answers.append(ground_truth)\n","\n","    # Calculate BLEU score using the predictions and the ground truths\n","    bleu_score = calculate_bleu(all_predictions, all_answers)\n","    return bleu_score\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T13:29:44.171574Z","iopub.status.busy":"2024-10-22T13:29:44.171274Z","iopub.status.idle":"2024-10-22T14:50:05.498761Z","shell.execute_reply":"2024-10-22T14:50:05.497740Z","shell.execute_reply.started":"2024-10-22T13:29:44.171536Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/630 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Training:   0%|          | 1/630 [00:14<2:32:56, 14.59s/it, loss=25.3]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","Training: 100%|██████████| 630/630 [14:52<00:00,  1.42s/it, loss=0.034] \n","Validating: 100%|██████████| 135/135 [01:18<00:00,  1.72it/s, loss=0.0303]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.4777, Validation Loss: 0.0359\n","Model saved!\n","**************************************************\n","Epoch 2/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [14:44<00:00,  1.40s/it, loss=0.012]  \n","Validating: 100%|██████████| 135/135 [01:18<00:00,  1.72it/s, loss=0.0105] \n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0235, Validation Loss: 0.0113\n","Model saved!\n","**************************************************\n","Epoch 3/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [14:41<00:00,  1.40s/it, loss=0.0184] \n","Validating: 100%|██████████| 135/135 [01:18<00:00,  1.72it/s, loss=0.00881]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0115, Validation Loss: 0.0096\n","Model saved!\n","**************************************************\n","Epoch 4/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [14:42<00:00,  1.40s/it, loss=0.0077] \n","Validating: 100%|██████████| 135/135 [01:18<00:00,  1.73it/s, loss=0.0081] \n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0092, Validation Loss: 0.0087\n","Model saved!\n","**************************************************\n","Epoch 5/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [14:39<00:00,  1.40s/it, loss=0.00385]\n","Validating: 100%|██████████| 135/135 [01:18<00:00,  1.72it/s, loss=0.00792]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0079, Validation Loss: 0.0082\n","Model saved!\n","**************************************************\n"]}],"source":["# Training loop\n","num_epochs = 5\n","best_loss = float('inf')\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    train_loss = train(model, train_loader, optimizer, device)\n","    val_loss = validate(model, val_loader, device)\n","    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n","    if val_loss < best_loss:\n","        best_loss = val_loss\n","        torch.save(model.state_dict(), 't5_qa_model.pth')\n","        print(\"Model saved!\")\n","    else:\n","        print(\"Validation Loss Increased. Model Not Saved.\")\n","    print(\"*\" * 50)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T14:50:05.500462Z","iopub.status.busy":"2024-10-22T14:50:05.500237Z","iopub.status.idle":"2024-10-22T14:50:05.506121Z","shell.execute_reply":"2024-10-22T14:50:05.505179Z","shell.execute_reply.started":"2024-10-22T14:50:05.500428Z"},"trusted":true},"outputs":[],"source":["# Calculate BLEU score\n","def calculate_bleu(predictions, references):\n","    bleu_scores = []\n","    for pred, ref in zip(predictions, references):\n","        bleu_scores.append(sentence_bleu([ref.split()], pred.split()))\n","    return sum(bleu_scores) / len(bleu_scores)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T14:50:05.507703Z","iopub.status.busy":"2024-10-22T14:50:05.507416Z","iopub.status.idle":"2024-10-22T14:50:05.518454Z","shell.execute_reply":"2024-10-22T14:50:05.517491Z","shell.execute_reply.started":"2024-10-22T14:50:05.507668Z"},"trusted":true},"outputs":[],"source":["# Test function\n","def test(model, test_loader, tokenizer, device):\n","    model.eval()\n","    all_predictions = []\n","    all_answers = []\n","    progress_bar = tqdm(test_loader, desc=\"Testing\")\n","    with torch.no_grad():\n","        for batch in progress_bar:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)  # Ensure answers are on the correct device\n","\n","            # Create decoder_input_ids from input_ids\n","            # This is typically done by prepending a start token. Adjust as needed.\n","            decoder_input_ids = torch.full((input_ids.shape[0], 1), tokenizer.pad_token_id, device=device)\n","            decoder_input_ids = torch.cat((decoder_input_ids, input_ids[:, :-1]), dim=1)  # Shift input_ids\n","\n","            # Pass through the model\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n","            \n","            # Get the logits from the model output\n","            logits = outputs.logits\n","            \n","            # Get the predicted token ids\n","            predicted_ids = torch.argmax(logits, dim=-1)\n","\n","            for i in range(predicted_ids.shape[0]):\n","                prediction = tokenizer.decode(predicted_ids[i], skip_special_tokens=True)\n","                all_predictions.append(prediction)\n","                all_answers.append(tokenizer.decode(labels[i], skip_special_tokens=True))  # Decode your answers\n","\n","    bleu_score = calculate_bleu(all_predictions, all_answers)\n","    return bleu_score\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-22T14:50:05.520460Z","iopub.status.busy":"2024-10-22T14:50:05.519897Z","iopub.status.idle":"2024-10-22T14:53:37.918135Z","shell.execute_reply":"2024-10-22T14:53:37.916628Z","shell.execute_reply.started":"2024-10-22T14:50:05.520400Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Testing: 100%|██████████| 135/135 [03:31<00:00,  1.57s/it]\n","/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["BLEU Score: 0.1875\n"]}],"source":["# Test the model\n","bleu_score = test(model, test_loader, tokenizer, device)\n","print(f\"BLEU Score: {bleu_score:.4f}\")\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5883900,"sourceId":9636743,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
