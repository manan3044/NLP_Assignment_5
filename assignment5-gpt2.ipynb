{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-17T12:16:03.270682Z","iopub.status.busy":"2024-10-17T12:16:03.269812Z","iopub.status.idle":"2024-10-17T12:16:11.267079Z","shell.execute_reply":"2024-10-17T12:16:11.266104Z","shell.execute_reply.started":"2024-10-17T12:16:03.270629Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","from nltk.translate.bleu_score import sentence_bleu\n","import nltk\n","\n","nltk.download('punkt')\n","\n","import logging\n","logging.disable(logging.WARNING)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:16:11.269695Z","iopub.status.busy":"2024-10-17T12:16:11.269128Z","iopub.status.idle":"2024-10-17T12:16:11.279808Z","shell.execute_reply":"2024-10-17T12:16:11.278881Z","shell.execute_reply.started":"2024-10-17T12:16:11.269644Z"},"trusted":true},"outputs":[],"source":["def load_coqa_data(file_path):\n","    with open(file_path, 'r') as f:\n","        data = json.load(f)\n","    return data['data']\n","\n","class CoQADataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        context = item['story']\n","        question = item['questions'][0]['input_text']\n","        answer = item['answers'][0]['input_text']\n","\n","        # Prepare the input text for GPT-2\n","        input_text = f\"Context: {context} Question: {question} Answer:\"\n","        target_text = f\"{answer}\"\n","\n","        # Tokenize the input and target texts\n","        inputs = self.tokenizer.encode_plus(\n","            input_text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        targets = self.tokenizer.encode_plus(\n","            target_text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'labels': targets['input_ids'].flatten(),\n","        }"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:17:22.340957Z","iopub.status.busy":"2024-10-17T12:17:22.340691Z","iopub.status.idle":"2024-10-17T12:17:23.596128Z","shell.execute_reply":"2024-10-17T12:17:23.595231Z","shell.execute_reply.started":"2024-10-17T12:17:22.340924Z"},"trusted":true},"outputs":[],"source":["data = load_coqa_data('/kaggle/input/coqa-train-v1.0.json')\n","train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n","val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:17:26.971579Z","iopub.status.busy":"2024-10-17T12:17:26.970544Z","iopub.status.idle":"2024-10-17T12:17:28.293976Z","shell.execute_reply":"2024-10-17T12:17:28.292911Z","shell.execute_reply.started":"2024-10-17T12:17:26.971531Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69b67ec7f0ab4f72ace54004a2101a79","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39f4e8a0fe014fb48e622fe7a497705c","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f5f56388b6f49129462ec31cbfd9604","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe86245f26194e76afe112584fb87be6","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18d0955f2c8e4e55a55f7eeda5986786","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# Initialize tokenizer and model\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","train_dataset = CoQADataset(train_data, tokenizer)\n","val_dataset = CoQADataset(val_data, tokenizer)\n","test_dataset = CoQADataset(test_data, tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8)\n","test_loader = DataLoader(test_dataset, batch_size=8)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:17:28.503420Z","iopub.status.busy":"2024-10-17T12:17:28.502586Z","iopub.status.idle":"2024-10-17T12:17:28.511076Z","shell.execute_reply":"2024-10-17T12:17:28.509958Z","shell.execute_reply.started":"2024-10-17T12:17:28.503377Z"},"trusted":true},"outputs":[],"source":["def train(model, train_loader, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","    progress_bar = tqdm(train_loader, desc=\"Training\")\n","\n","    for batch in progress_bar:\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass with GPT-2\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","\n","        # Get the loss from the output\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        # Backward pass and optimization step\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update progress bar with current loss\n","        progress_bar.set_postfix({'loss': loss.item()})\n","\n","    return total_loss / len(train_loader)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:17:30.071956Z","iopub.status.busy":"2024-10-17T12:17:30.071674Z","iopub.status.idle":"2024-10-17T12:17:34.042995Z","shell.execute_reply":"2024-10-17T12:17:34.042221Z","shell.execute_reply.started":"2024-10-17T12:17:30.071918Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41836ddf06ac4cae92758cfea83e10d3","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a620182c68df43598d70dec647d05427","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.resize_token_embeddings(len(tokenizer))\n","\n","\n","# Set device and move model to device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Set optimizer\n","optimizer = AdamW(model.parameters(), lr=5e-5)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:17:34.044787Z","iopub.status.busy":"2024-10-17T12:17:34.044558Z","iopub.status.idle":"2024-10-17T12:17:34.051558Z","shell.execute_reply":"2024-10-17T12:17:34.050767Z","shell.execute_reply.started":"2024-10-17T12:17:34.044753Z"},"trusted":true},"outputs":[],"source":["def validate(model, val_loader, device):\n","    model.eval()\n","    total_loss = 0\n","    progress_bar = tqdm(val_loader, desc=\"Validating\")\n","    with torch.no_grad():\n","        for batch in progress_bar:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            # Forward pass with GPT-2\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            progress_bar.set_postfix({'loss': loss.item()})\n","\n","    return total_loss / len(val_loader)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T13:30:34.212832Z","iopub.status.busy":"2024-10-17T13:30:34.212087Z","iopub.status.idle":"2024-10-17T13:30:34.220244Z","shell.execute_reply":"2024-10-17T13:30:34.219330Z","shell.execute_reply.started":"2024-10-17T13:30:34.212793Z"},"trusted":true},"outputs":[],"source":["# Test function\n","def test(model, test_loader, tokenizer, device):\n","    model.eval()\n","    all_predictions = []\n","    all_answers = []\n","    progress_bar = tqdm(test_loader, desc=\"Testing\")\n","    with torch.no_grad():\n","        for batch in progress_bar:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","\n","            # Generate answer using GPT-2 with max_new_tokens\n","            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=50, num_beams=5, early_stopping=True)\n","            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","            # Collect the predictions\n","            all_predictions.append(prediction)\n","\n","            # You need to decode the true answer for BLEU score calculation\n","            true_answer = tokenizer.decode(batch['labels'][0], skip_special_tokens=True)\n","            all_answers.append(true_answer)\n","\n","    bleu_score = calculate_bleu(all_predictions, all_answers)\n","    return bleu_score\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T12:17:36.821583Z","iopub.status.busy":"2024-10-17T12:17:36.820943Z","iopub.status.idle":"2024-10-17T13:29:55.865171Z","shell.execute_reply":"2024-10-17T13:29:55.864267Z","shell.execute_reply.started":"2024-10-17T12:17:36.821529Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [13:33<00:00,  1.29s/it, loss=0.0819]\n","Validating: 100%|██████████| 135/135 [00:53<00:00,  2.52it/s, loss=0.0735]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0996, Validation Loss: 0.0505\n","Model saved!\n","**************************************************\n","Epoch 2/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [13:34<00:00,  1.29s/it, loss=0.0693]\n","Validating: 100%|██████████| 135/135 [00:53<00:00,  2.54it/s, loss=0.0725]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0505, Validation Loss: 0.0499\n","Model saved!\n","**************************************************\n","Epoch 3/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [13:33<00:00,  1.29s/it, loss=0.0488]\n","Validating: 100%|██████████| 135/135 [00:53<00:00,  2.54it/s, loss=0.07]  \n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0488, Validation Loss: 0.0505\n","Validation Loss Increased. Model Not Saved.\n","**************************************************\n","Epoch 4/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [13:34<00:00,  1.29s/it, loss=0.0486]\n","Validating: 100%|██████████| 135/135 [00:53<00:00,  2.54it/s, loss=0.0706]\n"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0478, Validation Loss: 0.0512\n","Validation Loss Increased. Model Not Saved.\n","**************************************************\n","Epoch 5/5\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 630/630 [13:34<00:00,  1.29s/it, loss=0.0314]\n","Validating: 100%|██████████| 135/135 [00:53<00:00,  2.54it/s, loss=0.0748]"]},{"name":"stdout","output_type":"stream","text":["Train Loss: 0.0470, Validation Loss: 0.0515\n","Validation Loss Increased. Model Not Saved.\n","**************************************************\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["num_epochs = 5\n","best_loss = float('inf')\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    train_loss = train(model, train_loader, optimizer, device)\n","    val_loss = validate(model, val_loader, device)\n","    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n","    if val_loss < best_loss:\n","        best_loss = val_loss\n","        torch.save(model.state_dict(), 'gpt2_qa_model.pth')\n","        print(\"Model saved!\")\n","    else:\n","        print(\"Validation Loss Increased. Model Not Saved.\")\n","    print(\"*\" * 50)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T13:30:37.476519Z","iopub.status.busy":"2024-10-17T13:30:37.475998Z","iopub.status.idle":"2024-10-17T13:30:37.481832Z","shell.execute_reply":"2024-10-17T13:30:37.480985Z","shell.execute_reply.started":"2024-10-17T13:30:37.476478Z"},"trusted":true},"outputs":[],"source":["# Calculate BLEU score\n","def calculate_bleu(predictions, references):\n","    bleu_scores = []\n","    for pred, ref in zip(predictions, references):\n","        bleu_scores.append(sentence_bleu([ref.split()], pred.split()))\n","    return sum(bleu_scores) / len(bleu_scores)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-17T13:30:38.773962Z","iopub.status.busy":"2024-10-17T13:30:38.773336Z","iopub.status.idle":"2024-10-17T13:34:41.767336Z","shell.execute_reply":"2024-10-17T13:34:41.766411Z","shell.execute_reply.started":"2024-10-17T13:30:38.773926Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Testing: 100%|██████████| 135/135 [04:02<00:00,  1.80s/it]\n","/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["BLEU Score: 0.1059\n"]}],"source":["# Test the model\n","bleu_score = test(model, test_loader, tokenizer, device)\n","print(f\"BLEU Score: {bleu_score:.4f}\")\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\manan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["* Running on local URL:  http://127.0.0.1:7860\n","* Running on public URL: https://bfd70b7a8cf395f813.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["<div><iframe src=\"https://bfd70b7a8cf395f813.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, DistilBertForQuestionAnswering, DistilBertTokenizer, T5ForConditionalGeneration, T5Tokenizer\n","import gradio as gr\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def load_models():\n","    gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n","    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","    gpt2_model.load_state_dict(torch.load('gpt2_qa_model.pth', map_location=torch.device('cpu')))\n","    gpt2_model.eval()\n","\n","    distilbert_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n","    distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","    distilbert_model.load_state_dict(torch.load('distilbert_qa_model.pth', map_location=torch.device('cpu')))\n","    distilbert_model.eval()\n","\n","    t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n","    t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","    t5_model.load_state_dict(torch.load('t5_qa_model.pth', map_location=torch.device('cpu')), strict=False)\n","    t5_model.eval()\n","\n","    return (gpt2_model, gpt2_tokenizer), (distilbert_model, distilbert_tokenizer), (t5_model, t5_tokenizer)\n","\n","gpt2, distilbert, t5 = load_models()\n","\n","def gpt2_inference(article, question):\n","    input_text = f\"Context: {article}\\nQuestion: {question}\\nProvide a clear and concise answer:\"\n","\n","    inputs = gpt2[1].encode(input_text, return_tensors='pt').to(device)\n","    \n","    outputs = gpt2[0].generate(\n","        inputs,\n","        max_new_tokens=50,\n","        do_sample=True,\n","        temperature=0.7,\n","        pad_token_id=gpt2[1].eos_token_id,\n","        num_return_sequences=1,\n","        top_k=50,  \n","        top_p=0.95  \n","    )\n","    \n","    generated_answer = gpt2[1].decode(outputs[0], skip_special_tokens=True)\n","    \n","    answer_start = generated_answer.find(\"Provide a clear and concise answer:\") + len(\"Provide a clear and concise answer:\")\n","    answer = generated_answer[answer_start:].strip()\n","\n","    if \"?\" in answer:\n","        return \"Answer not available or too brief.\"\n","\n","    return answer\n","\n","\n","def distilbert_inference(article, question):\n","    inputs = distilbert[1].encode_plus(question, article, add_special_tokens=True, return_tensors='pt')\n","    with torch.no_grad():\n","        outputs = distilbert[0](**inputs)\n","    answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n","    answer_start = torch.argmax(answer_start_scores)\n","    answer_end = torch.argmax(answer_end_scores) + 1\n","    answer = distilbert[1].convert_tokens_to_string(distilbert[1].convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n","    return answer.strip()\n","\n","\n","def t5_inference(article, question):\n","    input_text = f\"question: {question} context: {article}\"\n","    inputs = t5[1].encode(input_text, return_tensors='pt')\n","    outputs = t5[0].generate(inputs, max_new_tokens=50, num_beams=5, early_stopping=True)\n","    return t5[1].decode(outputs[0], skip_special_tokens=True)\n","\n","\n","def inference(selected_model, article, question):\n","\n","    # Normalize the input\n","    selected_model = selected_model.strip().lower()\n","    print(f\"Normalized Model: '{selected_model}'\") \n","\n","    if selected_model == \"gpt-2\":\n","        return gpt2_inference(article, question)\n","    elif selected_model == \"distilbert\":\n","        return distilbert_inference(article, question)\n","    elif selected_model == \"t5\":\n","        return t5_inference(article, question)\n","    else:\n","        return \"Model not recognized.\"\n","    \n","\n","iface = gr.Interface(\n","    fn=inference,\n","    inputs=[\n","        gr.Dropdown(choices=[\"GPT-2\", \"DistilBERT\", \"T5\"], label=\"Select Model\", value=\"GPT-2\"),  # Set default value\n","        gr.Textbox(lines=2, placeholder=\"Enter the article here\"),\n","        gr.Textbox(lines=2, placeholder=\"Enter your question here\")\n","    ],\n","    outputs=\"text\",\n","    title=\"Question Answering with DistilBert, T5 and GPT2 Models\",\n","    description=\"Choose a model to answer questions based on the provided article.\"\n",")\n","\n","iface.launch(share=True)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5883900,"sourceId":9636743,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
